<!doctype html>
<html>

<head>

    <meta charset="utf-8">
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">

    <title>Emotion Recognition with Spatio-Tempora Neural Networks</title>

    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    
    

</head>

<body>

    <div class="section-blog-post">
        <div class="container">
            <div class="row">
                <div class="col-sm-12">
                    <div class="post-container">
                        <a class="back" href="./dataviz.html">Back to all posts</a>

                        <h1>Emotion Recognition with Spatio-Temporal Neural Networks</h1>

                        <span class="by">Posted by <strong>Balaganesh</strong> on <strong>July, 2021,</strong> 10 min read</span>
                        <span style="display:block; height: 25px;"></span>
                        <div class="featured-image">
                            <img src="images/distance-art/impression.jpg"
                                width="50%" , class="center_medium">
                        </div>
                        <span style="display:block; height: 25px;"></span>
                        <div class="post-body">
                            <p>
                                This article is a short explanation of the procedure and results of the work done during my master thesis 
                                <i>Emotion Recognition with Spatio-Tempora Neural Networks</i>. We try to cover as many things as possible in a 
                                concise manner given the thesis was 5 months long and it is not feasible to write about everything in this short post. 
                                In fact, the entire pipeline will be explored in two different parts. Let's call this post part 1, where we introduce the data,
                                the problem set and part of the methodology.                    
                           
                            </p>

                            <h2>Affective Computing</h2>
                            <span style="display:block; height: 25px;"></span>
                            <p>
                            The study of systems that can understand and simulate human emotions
                            in both spatial and temporal space is known as affective computing. Affective computing is a subset of the larger field of human-computer
                             interaction, which is multidisciplinary in nature. In the past few years with the advancements in computer hard-
                             ware, data availability and a robust annotation infrastructure, the functionality of deep learning techniques especially in the line of computer vision has
                             improved remarkably. These changes made the existence of affective systems
                             possible with emotion/expression detection being a cornerstone in multiple
                             domains of human computer interface like, healthcare, autonomous driving
                             and customer service.
                            </p>
                            <p>
                                In the context of this research, emotion is described as a state in which human
                                beings reveal their instinctive feelings through facial expressions. There are several type of emotion 
                                revealed through various streams or medium. The most common representation of the type of emotion is with the wheel of emotion
                                picture. There are 27 widely accepted emotions that appear on a gradient, but for most deep learning problems,
                                only the emotions in the inner concentric circles are used. 
                                <span style="display:block; height: 25px;"></span>
                                <img src="images/emotion/emotion-wheel.jpg  "
                                width="50%" , class="x",id='x'>
                            </p>

                            <p>
                                Another dimension of non triviality added to emotion recognition is the inclusion of temporal data. Classifying the emotion of a single frame is rather
                                trivial since training a deep neural network for image recognition is well researched and highly capable methods like transfer learning 
                                 make it simple. However, the display of emotions are on a gradient that includes multiple frames over a time period, 
                                 essentially making it a video recognition problem which brings several complications including computational complexity
                                  for training the model, as well as different dynamics, in which various emotional states are depicted with different intensity levels.
                            </p>

                            <h2>Approach</h2>
                            <span style="display:block; height: 25px;"></span>
                            <p>
                                In this research thesis a exploratory analysis of two different novel architectures for expression classification are investigated. The first one is a new
                                method to train 3D CNN’s with a 2D CNN’s complexity by simplifying the architecture to classify temporal features with two simple steps of <i>shift</i> and
                                <i>multiply-accumulate</i>. Second one is a radically newer approach in computer vision by making use of transformer networks for training video recognition.
                                Explaining both the methods would be rather long, so we will divide it into two posts, one for each method. 
                            </p>

                            <h2>Data</h2>
                            <span style="display:block; height: 25px;"></span>
                            <p>    
                            Two completely different datasets representing the categories of posed and spontaneous emotional display were explored.
                            The posed data is represented by <i><b>CREMA-D</b></i>, which also serves as a baseline for the
                            Spatio-temporal model’s ability to detect and recognise emotional features in
                            videos. <i><b>In the wild v2</b></i> or, in short, AffWild was used for the more difficult
                            spontaneous data. The pre-processing techniques used in each dataset are
                            similar, with some additional steps included in the AffWild data.
                            </p>    

                            <h3>CREMA-D</h3>

                            <p>
                                The Crowd Sourced Emotional Multi modal Actors Dataset (CREMA-D) contains 7422 short clips of 91 different actors portraying six different emotions.
                                The clips’ content is a carefully curated selection of 12 English sentences, each
                                of which is enacted by actors portraying one of the six emotions: Anger, Disgust, Fear, Happiness, Neutrality, and Sadness. Crowd-sourcing was used to
                                label the dataset, where 2433 people manually assigned labels to each clip.
                                The entire dataset contains both audio and visual data, but only the visual modality is examined in this study. Now let's take a look at some sample frames
                                from the CREMA-D dataset.
                                <span style="display:block; height: 25px;"></span>
                                <img src="images/emotion/cremad.png  "
                                width="50%" , class="center_medium">

                            </p>

                           
